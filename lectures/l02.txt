6.824 2013 Lecture 2: Infrastructure: RPC and threads

Remote Procedure Call (RPC)
  a key piece of distrib sys machinery; you'll see in lab 1
  goal: easy-to-program network communication
    hides most details of client/server communication
    client call is much like ordinary procedure call
    server handlers are much like ordinary procedures
  RPC is widely used!

RPC ideally makes net communication look just like fn call:
  Client:
    z = fn(x, y)
  Server:
    fn(x, y) {
      compute
      return z
    }
  RPC aims for this level of transparency

RPC message diagram:
  Client             Server
    request--->
       <---response

Software structure
  client app         handlers
    stubs           dispatcher
   RPC lib           RPC lib
     net  ------------ net
 
A few details:
  Marshalling: format data into packets
    Tricky for arrays, pointers, objects, &c
    Go's RPC library is pretty powerful!
  Binding: how does client know how to talk to the server?
    Might be a name service -- e.g. DNS
  Threads:
    Client often has many threads, so > 1 call outstanding, match up replies
    Handlers may be slow, so server often runs each in a thread

Look at lab-1.go handout (from last lecture):
  contains some fragments from the lab-1 code we give you
  main() shows application view: setup, then fn calls
  "Clerk" is client-side code that knows how to talk to specific server
    thus not quite like an ordinary fn call
  clerk.Lock() is a stub: turns fn call into RPC junk
    you'll modify Lock to try the backup if the primary fails
  LockServer holds the application-level state (lock table)
  LockServer.Lock() is a handler
    contains app-level server code
    RPC library calls it
    you'll modify Lock() to forward to the backup

Hard challenge: failures
  network may drop or delay packets
  network may be temporarily or permanently broken
  server may crash, maybe re-start
  can we hide this from client applications and server handlers?

Basic choice in RPC library design:
  Punt failures to application?
    this makes RPC hard to use (no longer like fn call)
  Hide failures from application?
    pretty convenient if possible
    but what about e.g. permanent server failure?

Let's talk about how RPC can hide failure from application
  suppose network discards the request packet
  what will client RPC library observe?
  what should the client RPC library do?
  how long should it wait before rxmt?

Now suppose the network delivered request, but discarded response
  what will client RPC library observe?
  what should it do?

Client re-sending leads to "at-least-once" behavior

Would at-least-once RPC cause trouble for a non-replicated lock server?
  bad case: [arrow diagram]
    send Lock(), response lost
    timeout, RPC re-sends Lock(), server will say "no"
  another bad case:
    send Lock
    send Unlock, network delays it
    RPC re-sends Unlock, received
    Lock again
    now first Unlock delivered, incorrectly releases lock

Is at-least-once ever OK?
  yes: if no side effects -- read-only operations
  yes: if application has its own plan for detecting duplicates

How can RPC system provide better behavior?
  server RPC library detects duplicate request
    returns previous reply instead of re-running handler
  "at most once"
  client includes unique ID (UID) with each request
    uses same UID for re-send
  server:
    if seen[uid]:
      r = old[uid]
    else
      r = handler()
      old[uid] = r
      seen[uid] = true

some at-most-once complexities
  how to ensure UID is unique?
    big random number?
    combine unique client ID (ip address?) with sequence #?
  server must eventually discard info about old RPCs
    when is discard safe?
    idea:
      unique client IDs
      per-client RPC sequence numbers
      client includes "seen all replies <= X" with every RPC
      much like TCP sequence #s and acks
    or only allow client one outstanding RPC at a time
      arrival of seq+1 allows server to discard all <= seq
    or client agrees to keep retrying for < 5 minutes
      server discards after 5+ minutes
  how to handle dup req while original is still executing?
    server doesn't know reply yet; don't want to run twice
    idea: "pending" flag per executing RPC; wait or ignore

What if the server crashes?
  if at-most-once duplicate info in memory, server will forget
    and accept duplicate requests
  maybe it should write the duplicate info to disk?
  maybe replica server should also replicate duplicate info?

What about "exactly once"?
  at-most-once plus unbounded retries

Go RPC is "at-most-once"
  open TCP connection
  write request to TCP connection
  TCP may retransmit, but server's TCP will filter out duplicates
  no retry in Go code (i.e. will NOT create 2nd TCP connection)
  Go RPC code returns an error if it doesn't get a reply
    perhaps after a timeout (from TCP)
    perhaps server didn't see request
    perhaps server processed request but server/net failed before reply came back

Go RPC's at-most-once isn't enough for Lab 1
  it only applies to a single RPC call
  if primary fails, your client will have to re-send to backup
    backup may have already seen forwarded request from primary
  Go RPC can't detect this kind of duplicate

***

Now: threads
  threads are a fundamental server structuring tool
  you'll use them a lot in the labs
  they can be tricky

Thread = "thread of control"
  threads allow one program to (logically) do many things at once
  the threads share memory
  each thread includes some per-thread state:
    program counter
    stack pointer
    register set
    stack
    maybe state in the o/s (e.g. what it is waiting for)

Example: suppose you want to send RPCs to many servers:

  var reply1 int
  var reply2 int
  go func() {
    reply1 = rpc to server A
  }()
  go func() {
    reply2 = rpc to server B
  }()

Note:
  go ... starts a new thread
  go's argument is a function call
  anonymous functions
  when the function returns, the thread disappears
  reply1 and reply2 are accessible in threads

Why use threads?
  exploit several processors to run an application faster
  interleave different activities in a convenient way
    one thread computes while another waits for disk read
    interleave background spell-check w/ foreground editing
  you can view the last two as merely a structuring convenience
    so the programmer doesn't have to write the interleaved code
    but it's a big convenience!

Problem: races
  count := 0
  go func() {
    count++
  }()
  go func() {
    count++
  }()
    
Go mutexes (often called locks)
  mu sync.Mutex
  mu.Lock()
  mu.Unlock()
  code between Lock/Unlock often called "critical section"
  defer mu.Unlock()

Problem: threads often need to wait for each other
  Example: how to wait for those two RPCs to both finish?

Go channels
  c := make(chan int)
  c <- 1
  x := <- c
  <- c
  receiving waits until somebody sends
  sending waits until somebody receives
    thus no harm if reader is slower than writer

Channel example:
  c1 := make(chan bool)
  c2 := make(chan bool)
  go func() { ...; c1 <- true } ()
  go func() { ...; c2 <- true } ()
  <- c1
  <- c2

Problem: Deadlock
  go func() { mu1.Lock(); mu2.Lock(); ...
  go func() { mu2.Lock(); mu1.Lock(); ...
  Example:
    transfer(a, b) {
      a.lock()
      b.lock()
      ...
    }
    go func() { transfer("fred", "ivy") }
    go func() { transfer("ivy", "fred") }
  Distributed deadlock too:
    server A handler():
      a.Lock()
      send RPC to server B
    server B handler():
      b.Lock()
      send RPC to server A
    you will face this in the labs!
  how to fix?
    don't hold more than one lock
    or always acquire locks in the same order (e.g. for bank transfer)
  hard if locks are in different modules
    requires modules to understand each others' internals

Locking granularity
  one mutex for whole lock_server?
  suppose we found handlers were often waiting for that one mutex
    what are reasonable options?
    one mutex per client?
    one mutex per lock?
  if one mutex per lock
    still need one mutex to protect table of locks
  danger of many locks---deadlock and races 

mutexes themselves are usually pretty cheap
  much less than a microsecond if you don't have to wait

***

let's look at today's handout -- rpc-handout.go
  it's a toy RPC system
  illustrates threads, mutexes, channels
  it's a toy
    assumes connection already open
    only supports an integer arg, integer reply
    doesn't deal with errors

struct ToyClient
  client RPC state 
  mutex per ToyClient
  connection to server (e.g. TCP socket)
  xid -- unique ID per call, to match reply to caller
  pending[] -- multiple threads may call, need to find them
    chan on which caller is waiting

Call
  application calls reply := client.Call(procNum, arg)
  procNum indicates what function to run on server
  WriteRequest knows the format of an RPC msg
    basically just the arguments turned into bits in a packet
  Q: could we move "xid := tc.xid" outside the mutex?
     after all, we are not changing anything
  Q: do we need to write inside the mutex?
  note: Go says you are responsible for preventing concurrent map ops
    that's one reason the update to pending is locked

Listener
  runs as a background thread
  not quite right that it may need to wait on chan for caller

Q: what if reply comes back very quickly?
   could Listener() see reply before pending[xid] entry exists?
   or before caller is waiting for channel?

Dispatcher
  note that the Dispatcher echos the xid back to the client
    so that Listener knows which Call to wake up
  Q: why run the handler in a separate thread?

main()
  note registering handler in handlers[] 
  what will the program print?
